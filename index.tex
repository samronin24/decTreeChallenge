% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Decision Tree Challenge},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Decision Tree Challenge}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Feature Importance and Categorical Variable Encoding}
\author{}
\date{}
\begin{document}
\maketitle


\section{🌳 Decision Tree Challenge - Feature Importance and Variable
Encoding}\label{decision-tree-challenge---feature-importance-and-variable-encoding}

Let's assume we want to predict house prices and understand which
features matter most. The key question is: \textbf{How does encoding
categorical variables as numbers affect our understanding of feature
importance?}

\subsection{The Ames Housing Dataset 🏠}\label{the-ames-housing-dataset}

We are analyzing the Ames Housing dataset which contains detailed
information about residential properties sold in Ames, Iowa from 2006 to
2010. This dataset is perfect for our analysis because it contains a
categorical variable (like zip code) and numerical variables (like
square footage, year built, number of bedrooms).

\subsection{The Problem: ZipCode as Numerical vs
Categorical}\label{the-problem-zipcode-as-numerical-vs-categorical}

\textbf{Key Question:} What happens when we treat zipCode as a numerical
variable in a decision tree? How does this affect feature importance
interpretation?

\textbf{The Issue:} Zip codes (50010, 50011, 50012, 50013) are
categorical variables representing discrete geographic areas,
i.e.~neighborhoods. When treated as numerical, the tree might split on
``zipCode \textgreater{} 50012.5'' - which has no meaningful
interpretation for house prices. Zip codes are non-ordinal categorical
variables meaning they have no inherent order that aids house price
prediction (i.e.~zip code 99999 is not the priceiest zip code).

\subsection{Data Loading and Model
Building}\label{data-loading-and-model-building}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load libraries}
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(tidyverse))}
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(rpart))}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(rpart.plot, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)) \{}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"rpart.plot"}\NormalTok{, }\AttributeTok{repos =} \StringTok{"https://cran.rstudio.com/"}\NormalTok{)}
  \FunctionTok{library}\NormalTok{(rpart.plot)}
\NormalTok{\}}

\CommentTok{\# Load data}
\NormalTok{sales\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv"}\NormalTok{)}

\CommentTok{\# Prepare model data (treating zipCode as numerical)}
\NormalTok{model\_data }\OtherTok{\textless{}{-}}\NormalTok{ sales\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, }
\NormalTok{         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\CommentTok{\# Split data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{train\_indices }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(model\_data), }\FloatTok{0.8} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(model\_data))}
\NormalTok{train\_data }\OtherTok{\textless{}{-}}\NormalTok{ model\_data[train\_indices, ]}
\NormalTok{test\_data }\OtherTok{\textless{}{-}}\NormalTok{ model\_data[}\SpecialCharTok{{-}}\NormalTok{train\_indices, ]}

\CommentTok{\# Build decision tree}
\NormalTok{tree\_model }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(SalePrice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                    \AttributeTok{data =}\NormalTok{ train\_data,}
                    \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
                    \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{maxdepth =} \DecValTok{3}\NormalTok{, }
                                          \AttributeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                          \AttributeTok{minbucket =} \DecValTok{10}\NormalTok{))}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Model built with"}\NormalTok{, }\FunctionTok{sum}\NormalTok{(tree\_model}\SpecialCharTok{$}\NormalTok{frame}\SpecialCharTok{$}\NormalTok{var }\SpecialCharTok{==} \StringTok{"\textless{}leaf\textgreater{}"}\NormalTok{), }\StringTok{"terminal nodes}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Model built with 7 terminal nodes
\end{verbatim}

\subsection{Tree Visualization}\label{tree-visualization}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize tree}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{require}\NormalTok{(rpart.plot, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)) \{}
  \FunctionTok{rpart.plot}\NormalTok{(tree\_model, }
             \AttributeTok{type =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{extra =} \DecValTok{101}\NormalTok{,}
             \AttributeTok{fallen.leaves =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{digits =} \DecValTok{0}\NormalTok{,}
             \AttributeTok{cex =} \FloatTok{0.8}\NormalTok{,}
             \AttributeTok{main =} \StringTok{"Decision Tree (zipCode as Numerical)"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{plot}\NormalTok{(tree\_model, }\AttributeTok{uniform =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Decision Tree (zipCode as Numerical)"}\NormalTok{)}
  \FunctionTok{text}\NormalTok{(tree\_model, }\AttributeTok{use.n =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-r-1.pdf}}

\subsection{Feature Importance
Analysis}\label{feature-importance-analysis}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-r-1.pdf}}

\subsection{Critical Analysis: The Encoding
Problem}\label{critical-analysis-the-encoding-problem}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{⚠️ The Problem Revealed}, breakable, coltitle=black, colback=white, titlerule=0mm, opacityback=0, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, toptitle=1mm, bottomtitle=1mm, colframe=quarto-callout-warning-color-frame, opacitybacktitle=0.6, toprule=.15mm, left=2mm, arc=.35mm]

\textbf{What to note:} Our decision tree treated \texttt{zipCode} as a
numerical variable. This leads to zip code being unimportant. Not
surprisingly, because there is no reason to believe allowing splits like
``zipCode \textless{} 50012.5'' should be beneficial for house price
prediction. This false coding of a variable creates several problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Potentially Meaningless Splits:} A zip code of 50013 is not
  ``greater than'' 50012 in any meaningful way for house prices
\item
  \textbf{False Importance:} The algorithm assigns importance to zipCode
  based on numerical splits rather than categorical distinctions OR the
  importance of zip code is completely missed as numerical ordering has
  no inherent relationship to house prices.
\item
  \textbf{Misleading Interpretations:} We might conclude zipCode is not
  important when our intuition tells us it should be important (listen
  to your intuition).
\end{enumerate}

\textbf{The Real Issue:} Zip codes are categorical variables
representing discrete geographic areas. The numerical values have no
inherent order or magnitude relationship to house prices. These must be
modelled as categorical variables.

\end{tcolorbox}

\subsection{Proper Categorical Encoding: The
Solution}\label{proper-categorical-encoding-the-solution}

Now let's repeat the analysis with zipCode properly encoded as
categorical variables to see the difference.

\subsubsection{Categorical Encoding
Analysis}\label{categorical-encoding-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert zipCode to factor (categorical)}
\NormalTok{model\_data\_cat }\OtherTok{\textless{}{-}}\NormalTok{ model\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zipCode =} \FunctionTok{as.factor}\NormalTok{(zipCode))}

\CommentTok{\# Split data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{train\_indices\_cat }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(model\_data\_cat), }\FloatTok{0.8} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(model\_data\_cat))}
\NormalTok{train\_data\_cat }\OtherTok{\textless{}{-}}\NormalTok{ model\_data\_cat[train\_indices\_cat, ]}
\NormalTok{test\_data\_cat }\OtherTok{\textless{}{-}}\NormalTok{ model\_data\_cat[}\SpecialCharTok{{-}}\NormalTok{train\_indices\_cat, ]}

\CommentTok{\# Build decision tree with categorical zipCode}
\NormalTok{tree\_model\_cat }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(SalePrice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                        \AttributeTok{data =}\NormalTok{ train\_data\_cat,}
                        \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
                        \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{maxdepth =} \DecValTok{3}\NormalTok{, }
                                              \AttributeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                              \AttributeTok{minbucket =} \DecValTok{10}\NormalTok{))}

\CommentTok{\# Feature importance with categorical zipCode}
\NormalTok{importance\_cat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Feature =} \FunctionTok{names}\NormalTok{(tree\_model\_cat}\SpecialCharTok{$}\NormalTok{variable.importance),}
  \AttributeTok{Importance =} \FunctionTok{as.numeric}\NormalTok{(tree\_model\_cat}\SpecialCharTok{$}\NormalTok{variable.importance)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(Importance)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Importance\_Percent =} \FunctionTok{round}\NormalTok{(Importance }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Importance) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{\# Check if zipCode appears in tree}
\NormalTok{zipcode\_in\_tree }\OtherTok{\textless{}{-}} \StringTok{"zipCode"} \SpecialCharTok{\%in\%} \FunctionTok{names}\NormalTok{(tree\_model\_cat}\SpecialCharTok{$}\NormalTok{variable.importance)}
\ControlFlowTok{if}\NormalTok{(zipcode\_in\_tree) \{}
\NormalTok{  zipcode\_rank\_cat }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(importance\_cat}\SpecialCharTok{$}\NormalTok{Feature }\SpecialCharTok{==} \StringTok{"zipCode"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsubsection{Tree Visualization: Categorical
zipCode}\label{tree-visualization-categorical-zipcode}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize tree with categorical zipCode}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{require}\NormalTok{(rpart.plot, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)) \{}
  \FunctionTok{rpart.plot}\NormalTok{(tree\_model\_cat, }
             \AttributeTok{type =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{extra =} \DecValTok{101}\NormalTok{,}
             \AttributeTok{fallen.leaves =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{digits =} \DecValTok{0}\NormalTok{,}
             \AttributeTok{cex =} \FloatTok{0.8}\NormalTok{,}
             \AttributeTok{main =} \StringTok{"Decision Tree (zipCode as Categorical)"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{plot}\NormalTok{(tree\_model\_cat, }\AttributeTok{uniform =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Decision Tree (zipCode as Categorical)"}\NormalTok{)}
  \FunctionTok{text}\NormalTok{(tree\_model\_cat, }\AttributeTok{use.n =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-cat-r-1.pdf}}

\subsubsection{Feature Importance: Categorical
zipCode}\label{feature-importance-categorical-zipcode}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-cat-r-1.pdf}}

\subsection{Discussion Questions for
Challenge}\label{discussion-questions-for-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerical vs Categorical Encoding:} There are four models
  above, two in R and two in Python. For each language, the models
  differ by how zip code is modelled, either as a numerical variable or
  as a categorical variable. Given what you know about zip codes and
  real estate prices, how should zip code be modelled, numerically or
  categorically?
\end{enumerate}

\subsubsection{Given what I know about zip codes and real estate prices,
zip code should be modelled as categorical because when the zipcode is
treated as numerical, the tree ends up splitting for example ``zipcode
\textgreater50012.5''. This does not make sense because zip code of
50013 is not ``greater than'' 50012 in terms of house prices. Logially,
zip codes do play a significant role in prediciting house prices as zip
codes with more income tend to have houses that are worth more. On the
other hand, the right way to model zip code would be categorical where
we treat it as discrete cateogies. When zipcode is numerical, the
feature importance ranks zipcode very low whereas when zipcode is
categorical, there is high importance as it becomes one of the most
important features. Moreover, when zip codes are treated as categories,
the decision tree can split on ``zipcode=-50010'' vs ``zipcode==50010''
and make clear distinctions on different neightborhoods. In regards to
real estate prices, it would be a wrong business decision to use
numerical encoding since you would conclude location does not matter for
house prices. Therefore, data scientists should be very careful on how
they prepare their data as it can lead to a common mistake that makes
wrong business
insights.}\label{given-what-i-know-about-zip-codes-and-real-estate-prices-zip-code-should-be-modelled-as-categorical-because-when-the-zipcode-is-treated-as-numerical-the-tree-ends-up-splitting-for-example-zipcode-50012.5.-this-does-not-make-sense-because-zip-code-of-50013-is-not-greater-than-50012-in-terms-of-house-prices.-logially-zip-codes-do-play-a-significant-role-in-prediciting-house-prices-as-zip-codes-with-more-income-tend-to-have-houses-that-are-worth-more.-on-the-other-hand-the-right-way-to-model-zip-code-would-be-categorical-where-we-treat-it-as-discrete-cateogies.-when-zipcode-is-numerical-the-feature-importance-ranks-zipcode-very-low-whereas-when-zipcode-is-categorical-there-is-high-importance-as-it-becomes-one-of-the-most-important-features.-moreover-when-zip-codes-are-treated-as-categories-the-decision-tree-can-split-on-zipcode-50010-vs-zipcode50010-and-make-clear-distinctions-on-different-neightborhoods.-in-regards-to-real-estate-prices-it-would-be-a-wrong-business-decision-to-use-numerical-encoding-since-you-would-conclude-location-does-not-matter-for-house-prices.-therefore-data-scientists-should-be-very-careful-on-how-they-prepare-their-data-as-it-can-lead-to-a-common-mistake-that-makes-wrong-business-insights.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{R vs Python Implementation Differences:} When modelling zip
  code as a categorical variable, the output tree and feature importance
  differs quite significantly between R and Python. Investigate why this
  is the case. Which language would you say does a better job of
  modelling zip code as a categorical variable? Why is this the case? Do
  you see any documentation suggesting the other language does a better
  job? If so, please provide a quote from the documentation.
\end{enumerate}

\subsubsection{R and Python differ in categorical variable handling
because R's rpart package supports categorical variables and has an
algorithm that splits directly on categorical vairable and treates it as
a single feature for importance calculation. On the other hand, Python's
scikit-learn requires all input data to be numerical and must use
one-hot encoding, which creates seperaate binary columns for each zip
code. Therefore the importance is spread across multiple dummy
variables. R does a significantly better job of modeling categorical
variables because R creates meaningful splits that group related
cateogries together but Python creates artificial splits on individual
dummy variables that are harder to interpret. We can see in feature
importance that Python dilutes importance across multiple dummy
variables which is underestimating the true importance. R maintains
reasonable tree complexity with logical categorical splits whereas
Python creates overly complex trees with many splits on dummy variables.
To support my claim, I will provide a quote from the documentation of
the sklearn.tree.DecisionTreeRegression site. The quote says, ``However,
the scikit-learn implementation does not support categorical variables
for now.'' This explains why Python does a worse job of modelling
categorical
variables.}\label{r-and-python-differ-in-categorical-variable-handling-because-rs-rpart-package-supports-categorical-variables-and-has-an-algorithm-that-splits-directly-on-categorical-vairable-and-treates-it-as-a-single-feature-for-importance-calculation.-on-the-other-hand-pythons-scikit-learn-requires-all-input-data-to-be-numerical-and-must-use-one-hot-encoding-which-creates-seperaate-binary-columns-for-each-zip-code.-therefore-the-importance-is-spread-across-multiple-dummy-variables.-r-does-a-significantly-better-job-of-modeling-categorical-variables-because-r-creates-meaningful-splits-that-group-related-cateogries-together-but-python-creates-artificial-splits-on-individual-dummy-variables-that-are-harder-to-interpret.-we-can-see-in-feature-importance-that-python-dilutes-importance-across-multiple-dummy-variables-which-is-underestimating-the-true-importance.-r-maintains-reasonable-tree-complexity-with-logical-categorical-splits-whereas-python-creates-overly-complex-trees-with-many-splits-on-dummy-variables.-to-support-my-claim-i-will-provide-a-quote-from-the-documentation-of-the-sklearn.tree.decisiontreeregression-site.-the-quote-says-however-the-scikit-learn-implementation-does-not-support-categorical-variables-for-now.-this-explains-why-python-does-a-worse-job-of-modelling-categorical-variables.}

\textbf{What makes a great report:}

\begin{itemize}
\tightlist
\item
  \textbf{Clear narrative:} Tell the story of what you discovered about
  decision tree feature importance
\item
  \textbf{Insightful analysis:} Focus on the most interesting
  differences between numerical and categorical encoding
\item
  \textbf{Professional presentation:} Clean, readable, and engaging
\item
  \textbf{Concise conclusions:} No AI babble or unnecessary technical
  jargon
\item
  \textbf{Human insights:} Your interpretation of what the feature
  importance rankings actually mean (or don't mean)
\item
  \textbf{Documentation-based analysis:} For question 2, ground your
  analysis in actual library documentation
\end{itemize}

\textbf{What we're looking for:} A compelling 1-2 minute read that
demonstrates both the power of decision trees for interpretability and
the critical importance of proper variable encoding. :::

\subsubsection{Questions to Answer for 75\% Grade on
Challenge}\label{questions-to-answer-for-75-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerical vs Categorical Analysis:} Provide a clear,
  well-reasoned answer to question 1 about how zip codes should be
  modelled. Your answer should demonstrate understanding of why
  categorical variables need special treatment in decision trees.
\end{enumerate}

\subsubsection{Questions to Answer for 85\% Grade on
Challenge}\label{questions-to-answer-for-85-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{R vs Python Implementation Analysis:} Provide a thorough
  analysis of question 2, including investigation of the official
  documentation for both \texttt{rpart} (R) and
  \texttt{sklearn.tree.DecisionTreeRegressor} (Python). Your analysis
  should explain the technical differences and provide a reasoned
  opinion about which implementation handles categorical variables
  better.
\end{enumerate}

\subsubsection{Questions to Answer for 95\% Grade on
Challenge}\label{questions-to-answer-for-95-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Professional Presentation:} Your discussion answers should be
  written in a professional, engaging style that would be appropriate
  for a business audience. Avoid technical jargon and focus on practical
  implications. Use Quarto markdown linking to create a link to the
  discussion section from the top of the page (see
  https://quarto.org/docs/authoring/cross-references.html\#sections).
\end{enumerate}

\subsubsection{Questions to Answer for 100\% Grade on
Challenge}\label{questions-to-answer-for-100-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Documentation Integration:} For question 2, include a specific
  quote from the official documentation of
  \texttt{sklearn.tree.DecisionTreeRegressor} that supports your
  analysis.
\end{enumerate}




\end{document}
